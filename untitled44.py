# -*- coding: utf-8 -*-
"""Untitled44.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dqEAJyYlGOlNBUWUOcYmLSmD64nhoob9
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder, Binarizer


# 1. Load dataset

df = pd.read_csv("/content/basketball_shooting_dataset.csv")  # update with your actual file

print("Original shape:", df.shape)
print(df.head())

# 2. Handle duplicates & missing values

df = df.drop_duplicates()

for col in df.columns:
    if df[col].dtype in ['float64', 'int64']:
        df[col] = df[col].fillna(df[col].mean())
    else:
        df[col] = df[col].fillna(df[col].mode()[0])

print("After cleaning:", df.shape)


# 3. Encode categorical variables

# Encode Shot_Type if it exists
if "Shot_Type" in df.columns:
    le = LabelEncoder()
    df['Shot_Type'] = le.fit_transform(df['Shot_Type'])


# 4. Define features and target

# Use the correct target name
target_col = "Shot_Result"

# Drop non-predictive identifiers
X = df.drop([target_col, 'Player_ID', 'Timestamp'], axis=1, errors='ignore')
y = df[target_col]  # 1 = made, 0 = missed


# 5. Preprocessing for GaussianNB

scaler = StandardScaler()
X_gaussian = scaler.fit_transform(X)


# 6. Preprocessing for BernoulliNB

# Convert features to binary (threshold = median of each column)
X_binarized = X.apply(lambda col: (col > col.median()).astype(int))
X_bernoulli = X_binarized.values


# 7. Train-test split (50/50)

Xg_train, Xg_test, yg_train, yg_test = train_test_split(
    X_gaussian, y, test_size=0.5, random_state=42, stratify=y
)

Xb_train, Xb_test, yb_train, yb_test = train_test_split(
    X_bernoulli, y, test_size=0.5, random_state=42, stratify=y
)

print("Gaussian shapes:", Xg_train.shape, Xg_test.shape)
print("Bernoulli shapes:", Xb_train.shape, Xb_test.shape)

from sklearn.naive_bayes import GaussianNB, BernoulliNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix


# 8. Train Gaussian Naïve Bayes

gnb = GaussianNB()
gnb.fit(Xg_train, yg_train)

y_pred_g = gnb.predict(Xg_test)
print("\n=== Gaussian Naïve Bayes Results ===")
print("Accuracy:", accuracy_score(yg_test, y_pred_g))
print("Confusion Matrix:\n", confusion_matrix(yg_test, y_pred_g))
print("Classification Report:\n", classification_report(yg_test, y_pred_g))


# 9. Train Bernoulli Naïve Bayes

bnb = BernoulliNB()
bnb.fit(Xb_train, yb_train)

y_pred_b = bnb.predict(Xb_test)
print("\n=== Bernoulli Naïve Bayes Results ===")
print("Accuracy:", accuracy_score(yb_test, y_pred_b))
print("Confusion Matrix:\n", confusion_matrix(yb_test, y_pred_b))
print("Classification Report:\n", classification_report(yb_test, y_pred_b))

probs_g = gnb.predict_proba(Xg_test)
probs_b = bnb.predict_proba(Xb_test)
print(probs_g[:10])  # first 10 probability predictions

import numpy as np

# Get feature names
feature_names = X.columns.tolist()

# Analyze per shot type
shot_types = df['Shot_Type'].unique()

for st in shot_types:
    print(f"\n=== Analysis for Shot Type: {st} ===")

    # Filter dataset for this shot type
    mask = df['Shot_Type'] == st
    X_st = X_gaussian[mask]   # scaled features
    y_st = y[mask]

    if len(np.unique(y_st)) < 2:
        print(" Not enough variation in target for this shot type.")
        continue

    # Train GaussianNB
    gnb_st = GaussianNB()
    gnb_st.fit(X_st, y_st)

    # Inspect means and variances
    means = gnb_st.theta_   # class-conditional means
    variances = gnb_st.var_ # class-conditional variances

    # Compute importance score = abs(mean difference) / pooled std
    importance = np.abs(means[1] - means[0]) / np.sqrt((variances[0] + variances[1]) / 2)

    # Rank features by importance
    ranked = sorted(zip(feature_names, importance), key=lambda x: -x[1])

    print("Top 5 important features:")
    for feat, score in ranked[:5]:
        print(f"  {feat}: importance {score:.3f}")

def player_profile(age, height, weight):
    # Categorize age
    if age < 25:
        age_group = "Young"
    elif age <= 30:
        age_group = "Prime"
    else:
        age_group = "Veteran"

    # Categorize height
    if height < 185:
        height_group = "Short"
    elif height <= 195:
        height_group = "Medium"
    else:
        height_group = "Tall"

    # Categorize weight
    if weight < 75:
        weight_group = "Light"
    elif weight <= 90:
        weight_group = "Medium"
    else:
        weight_group = "Heavy"

    return age_group, height_group, weight_group


def recommend_training(age, height, weight):
    age_group, height_group, weight_group = player_profile(age, height, weight)

    recommendations = []

    # Height-based shot type preferences
    if height_group == "Tall":
        recommendations.append("Focus on Layups (close-range finishing).")
        recommendations.append("Improve Release Timing for contested shots.")
    elif height_group == "Medium":
        recommendations.append("Balanced training: Jump Shots + Set Shots.")
        recommendations.append("Work on Release Angle consistency.")
    else:  # Short players
        recommendations.append("Develop Jump Shots to offset height disadvantage.")
        recommendations.append("Enhance Shot Speed for quicker release.")

    # Weight-based suggestions
    if weight_group == "Light":
        recommendations.append("Leverage agility → focus on Jump Shots with strong wrist control.")
    elif weight_group == "Heavy":
        recommendations.append("Strength helps stability → practice Free Throws/Set Shots with Arc Radius control.")

    # Age-based focus
    if age_group == "Young":
        recommendations.append("Work on fundamentals: Shot Angle & Speed consistency.")
    elif age_group == "Veteran":
        recommendations.append("Focus on maintaining Follow-Through and Release Angle precision.")

    return {
        "Profile": f"{age_group}, {height_group}, {weight_group}",
        "Recommendations": recommendations
    }

# Example: player age=29, height=200, weight=93
print(recommend_training(29, 200, 93))

import os
import argparse
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_predict, cross_validate
from sklearn.preprocessing import StandardScaler, LabelEncoder, Binarizer
from sklearn.naive_bayes import GaussianNB, BernoulliNB
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import (accuracy_score, confusion_matrix, classification_report,
                             roc_auc_score, roc_curve, precision_recall_curve, average_precision_score)
from sklearn.inspection import permutation_importance
from sklearn.feature_selection import mutual_info_classif

import warnings
warnings.filterwarnings("ignore", category=UserWarning)


# Helper functions

def ensure_output_dir(path="outputs"):
    os.makedirs(path, exist_ok=True)
    return path

def safe_print_df(df, name="df", n=5):
    print(f"\n{name} (first {n} rows):")
    print(df.head(n))

def plot_confusion_matrix(cm, classes, title, outpath):
    plt.figure(figsize=(5,4))
    plt.imshow(cm, interpolation='nearest', aspect='auto')
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes)
    plt.yticks(tick_marks, classes)
    fmt = 'd'
    thresh = cm.max() / 2.
    for i, j in np.ndindex(cm.shape):
        plt.text(j, i, format(cm[i, j], fmt),
                 ha="center", va="center",
                 color="white" if cm[i, j] > thresh else "black")
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()
    plt.savefig(outpath)
    plt.close()

def plot_roc_pr(y_true, y_score, model_name, outdir):
    # ROC
    fpr, tpr, _ = roc_curve(y_true, y_score)
    roc_auc = roc_auc_score(y_true, y_score)
    plt.figure()
    plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.3f})')
    plt.plot([0, 1], [0, 1], linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve: {model_name}')
    plt.legend(loc='lower right')
    plt.grid(True)
    plt.savefig(os.path.join(outdir, f'roc_{model_name}.png'))
    plt.close()
    # Precision-Recall
    precision, recall, _ = precision_recall_curve(y_true, y_score)
    ap = average_precision_score(y_true, y_score)
    plt.figure()
    plt.step(recall, precision, where='post', label=f'AP = {ap:.3f}')
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title(f'Precision-Recall Curve: {model_name}')
    plt.legend(loc='lower left')
    plt.grid(True)
    plt.savefig(os.path.join(outdir, f'pr_{model_name}.png'))
    plt.close()
    return roc_auc, ap

def plot_probability_distributions(probs, y_true, model_name, outdir):
    # probs expected to be probability of positive class (shape n_samples,)
    made = probs[y_true == 1]
    missed = probs[y_true == 0]
    plt.figure()
    plt.hist(made, bins=25, alpha=0.7, label='Made (1)', density=True)
    plt.hist(missed, bins=25, alpha=0.7, label='Missed (0)', density=True)
    plt.xlabel('Predicted probability (positive class)')
    plt.ylabel('Density')
    plt.title(f'Predicted probability distribution: {model_name}')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(os.path.join(outdir, f'prob_dist_{model_name}.png'))
    plt.close()

def top_n_from_importance(names, scores, n=10):
    pairs = list(zip(names, scores))
    pairs_sorted = sorted(pairs, key=lambda x: -abs(x[1]))
    return pairs_sorted[:n]


# Main pipeline

def main(argv=None):
    parser = argparse.ArgumentParser()
    parser.add_argument('--data', type=str, required=True, help='Path to CSV dataset')
    parser.add_argument('--output', type=str, default='outputs', help='Directory to save outputs')
    args = parser.parse_args(argv)

    outdir = ensure_output_dir(args.output)

    # 1. Load dataset
    df = pd.read_csv(args.data)
    print("Original shape:", df.shape)
    safe_print_df(df, "Raw data")

    # Check essential columns
    target_col = "Shot_Result"
    if target_col not in df.columns:
        raise ValueError(f"Target column '{target_col}' not found in data.")

    # 2. Handle duplicates & missing values
    df = df.drop_duplicates()
    for col in df.columns:
        if df[col].dtype in ['float64', 'int64', 'int32', 'float32']:
            df[col] = df[col].fillna(df[col].mean())
        else:
            # if all NaN, mode may fail; fallback to empty string
            try:
                df[col] = df[col].fillna(df[col].mode()[0])
            except Exception:
                df[col] = df[col].fillna('')
    print("After cleaning shape:", df.shape)

    # 3. Encode categorical variables
    # example: Shot_Type
    if "Shot_Type" in df.columns:
        le = LabelEncoder()
        df['Shot_Type_encoded'] = le.fit_transform(df['Shot_Type'])
    else:
        df['Shot_Type_encoded'] = 0  # fallback

    # 4. Drop non-predictive identifiers and define X, y
    drop_cols = [target_col, 'Player_ID', 'Timestamp']  # errors='ignore' equivalent below
    X = df.drop(columns=[c for c in drop_cols if c in df.columns])
    # We keep Shot_Type_original text in df, but X contains Shot_Type_encoded if it exists
    # ensure we don't keep the original text column in X
    if 'Shot_Type' in X.columns:
        X = X.drop(columns=['Shot_Type'])
    y = df[target_col].astype(int).values

    # Feature names
    feature_names = X.columns.tolist()
    print("Features used:", feature_names)

    # 5. Preprocessing for GaussianNB (scale continuous features)
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    X_scaled_df = pd.DataFrame(X_scaled, index=X.index, columns=X.columns)

    # 6. Preprocessing for BernoulliNB (binarize using median per column)
    X_binarized_df = X.copy()
    for col in X.columns:
        # Only binarize numeric columns; for encoded Shot_Type_encoded already numeric
        if np.issubdtype(X[col].dtype, np.number):
            med = X[col].median()
            X_binarized_df[col] = (X[col] > med).astype(int)
        else:
            # If non-numeric remains, convert to 0/1 presence
            X_binarized_df[col] = (X[col].notnull()).astype(int)
    X_binarized = X_binarized_df.values

    # 7. Cross-validated model evaluation (StratifiedKFold)
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

    # Models to evaluate: GaussianNB (scaled), BernoulliNB (binarized), LogisticRegression (scaled), KNN (scaled)
    models = {
        'GaussianNB': (GaussianNB(), X_scaled_df, 'scaled'),
        'BernoulliNB': (BernoulliNB(), pd.DataFrame(X_binarized_df, index=X.index, columns=X.columns), 'binary'),
        'LogisticRegression': (LogisticRegression(max_iter=1000), X_scaled_df, 'scaled'),
        'KNN': (KNeighborsClassifier(n_neighbors=5), X_scaled_df, 'scaled')
    }

    summary_results = []

    for name, (model, X_model_df, dtype) in models.items():
        print("\n" + "="*40)
        print(f"Cross-validating model: {name}")
        X_vals = X_model_df.values
        # Get cross-validated predicted probabilities for positive class
        try:
            y_proba = cross_val_predict(model, X_vals, y, cv=skf, method='predict_proba', n_jobs=-1)
            y_proba_pos = y_proba[:, 1]
        except Exception as e:
            # Some models may not support predict_proba in this pipeline; fallback to decision_function or predict
            print(f"predict_proba failed for {name}: {e}. Falling back to predict.")
            y_pred_cv = cross_val_predict(model, X_vals, y, cv=skf, method='predict', n_jobs=-1)
            # approximate probabilities as binary predictions (0/1)
            y_proba_pos = y_pred_cv.astype(float)

        # Cross-validated predicted labels
        y_pred_cv = (y_proba_pos >= 0.5).astype(int)

        # Metrics
        acc = accuracy_score(y, y_pred_cv)
        try:
            auc = roc_auc_score(y, y_proba_pos)
        except Exception:
            auc = float('nan')
        ap = average_precision_score(y, y_proba_pos)

        print(f"{name} - Accuracy (cv): {acc:.4f} | ROC AUC (cv): {auc:.4f} | AvgPrecision (cv): {ap:.4f}")
        cm = confusion_matrix(y, y_pred_cv)
        print("Confusion matrix:")
        print(cm)
        print("Classification report:")
        print(classification_report(y, y_pred_cv, digits=4))

        # Save confusion matrix heatmap
        plot_confusion_matrix(cm, classes=['Miss (0)','Made (1)'],
                              title=f'{name} Confusion Matrix (cv)',
                              outpath=os.path.join(outdir, f'confusion_{name}.png'))

        # Save ROC & PR curves
        try:
            plot_roc_pr(y, y_proba_pos, name, outdir)
        except Exception as e:
            print("Could not plot ROC/PR:", e)

        # Save probability distribution
        plot_probability_distributions(y_proba_pos, y, name, outdir)

        # Fit model to full data for feature importance analyses (where applicable)
        fitted = model.fit(X_vals, y)

        # Permutation importance (on entire dataset; better would be separate holdout)
        try:
            perm_res = permutation_importance(fitted, X_vals, y, n_repeats=30, random_state=42, n_jobs=-1)
            perm_importances = perm_res.importances_mean
        except Exception as e:
            print("Permutation importance failed:", e)
            perm_importances = np.zeros(X_vals.shape[1])

        # Mutual information (between X_model_df and y)
        try:
            mi = mutual_info_classif(X_vals, y, discrete_features=(dtype=='binary'), random_state=42)
        except Exception as e:
            print("Mutual information failed:", e)
            mi = np.zeros(X_vals.shape[1])

        # Save top features by permutation and mutual info
        top_perm = top_n_from_importance(X_model_df.columns.tolist(), perm_importances, n=10)
        top_mi = top_n_from_importance(X_model_df.columns.tolist(), mi, n=10)

        perm_df = pd.DataFrame(top_perm, columns=['feature','perm_importance']).set_index('feature')
        mi_df = pd.DataFrame(top_mi, columns=['feature','mutual_info']).set_index('feature')
        perm_df.to_csv(os.path.join(outdir, f'top_perm_{name}.csv'))
        mi_df.to_csv(os.path.join(outdir, f'top_mi_{name}.csv'))

        print("\nTop permutation-importance features:")
        print(perm_df.head(10))
        print("\nTop mutual-information features:")
        print(mi_df.head(10))

        summary_results.append({
            'model': name,
            'accuracy_cv': acc,
            'roc_auc_cv': auc,
            'avg_precision_cv': ap,
            'confusion_matrix': cm,
            'top_perm': perm_df,
            'top_mi': mi_df
        })

    # 8. Train/test split for additional tests and per-shot-type analyses
    X_train_scaled, X_test_scaled, y_train, y_test, idx_train, idx_test = train_test_split(
        X_scaled_df.values, y, X.index, test_size=0.5, random_state=42, stratify=y
    )

    # Fit GaussianNB on training set & evaluate on test set (for detailed outputs)
    gnb = GaussianNB()
    gnb.fit(X_train_scaled, y_train)
    y_pred_g = gnb.predict(X_test_scaled)
    y_proba_g = gnb.predict_proba(X_test_scaled)[:, 1]

    print("\n=== Gaussian Naive Bayes (holdout test) ===")
    print("Accuracy:", accuracy_score(y_test, y_pred_g))
    print("ROC AUC:", roc_auc_score(y_test, y_proba_g))
    print("Classification report:\n", classification_report(y_test, y_pred_g))
    cm_g = confusion_matrix(y_test, y_pred_g)
    plot_confusion_matrix(cm_g, classes=['Miss(0)','Made(1)'],
                          title='GaussianNB Confusion Matrix (test)',
                          outpath=os.path.join(outdir, 'confusion_gaussian_test.png'))
    plot_roc_pr(y_test, y_proba_g, 'GaussianNB_test', outdir)
    plot_probability_distributions(y_proba_g, y_test, 'GaussianNB_test', outdir)

    # 9. Bernoulli on binarized train/test
    # Build binarized train/test using the earlier median rule (but match indices)
    X_binarized_df = pd.DataFrame(X_binarized_df, index=X.index, columns=X.columns)
    Xb_train = X_binarized_df.loc[idx_train].values
    Xb_test = X_binarized_df.loc[idx_test].values

    bnb = BernoulliNB()
    bnb.fit(Xb_train, y_train)
    y_pred_b = bnb.predict(Xb_test)
    y_proba_b = bnb.predict_proba(Xb_test)[:, 1]
    print("\n=== Bernoulli Naive Bayes (holdout test) ===")
    print("Accuracy:", accuracy_score(y_test, y_pred_b))
    try:
        print("ROC AUC:", roc_auc_score(y_test, y_proba_b))
    except Exception:
        print("ROC AUC: N/A")
    print("Classification report:\n", classification_report(y_test, y_pred_b))
    cm_b = confusion_matrix(y_test, y_pred_b)
    plot_confusion_matrix(cm_b, classes=['Miss(0)','Made(1)'],
                          title='BernoulliNB Confusion Matrix (test)',
                          outpath=os.path.join(outdir, 'confusion_bernoulli_test.png'))
    plot_roc_pr(y_test, y_proba_b, 'BernoulliNB_test', outdir)
    plot_probability_distributions(y_proba_b, y_test, 'BernoulliNB_test', outdir)

    # 10. Permutation importance on GaussianNB (test set)
    try:
        perm_g = permutation_importance(gnb, X_test_scaled, y_test, n_repeats=40, random_state=42, n_jobs=-1)
        perm_idx = np.argsort(-perm_g.importances_mean)
        perm_df = pd.DataFrame({
            'feature': X.columns[perm_idx],
            'perm_mean': perm_g.importances_mean[perm_idx]
        })
        perm_df.to_csv(os.path.join(outdir, 'gaussian_perm_test.csv'), index=False)
        print("\nTop permutation importance (GaussianNB on test):")
        print(perm_df.head(10))
    except Exception as e:
        print("Permutation importance on test failed:", e)

    # 11. Mutual information on full scaled X vs y
    try:
        mi_full = mutual_info_classif(X_scaled_df.values, y, discrete_features=False, random_state=42)
        mi_idx = np.argsort(-mi_full)
        mi_df = pd.DataFrame({
            'feature': X.columns[mi_idx],
            'mi': mi_full[mi_idx]
        })
        mi_df.to_csv(os.path.join(outdir, 'mutual_info_full.csv'), index=False)
        print("\nTop mutual information (full):")
        print(mi_df.head(10))
    except Exception as e:
        print("Mutual info failed:", e)

    # 12. Per-shot-type feature importance & visualization
    if 'Shot_Type_encoded' in df.columns:
        shot_types = df['Shot_Type_encoded'].unique()
        for st in shot_types:
            mask = df['Shot_Type_encoded'] == st
            if mask.sum() < 10:
                print(f"Skipping shot type {st} (too few samples: {mask.sum()})")
                continue
            X_st_scaled = X_scaled_df.loc[mask, :]
            y_st = y[mask]
            if len(np.unique(y_st)) < 2:
                print(f"Not enough target variation for shot type {st}")
                continue
            gnb_st = GaussianNB()
            gnb_st.fit(X_st_scaled.values, y_st)
            means = gnb_st.theta_  # shape (n_classes, n_features)
            variances = gnb_st.var_
            # importance as in your snippet: abs(mean difference)/pooled std
            if means.shape[0] >= 2:
                importance = np.abs(means[1] - means[0]) / np.sqrt((variances[0] + variances[1]) / 2 + 1e-9)
            else:
                importance = np.abs(means[0]) / (np.sqrt(variances[0] + 1e-9))
            ranked = sorted(zip(X.columns.tolist(), importance), key=lambda x: -x[1])
            ranked_top = ranked[:10]
            print(f"\n=== Shot Type {st} top features ===")
            for feat, score in ranked_top[:5]:
                print(f"  {feat}: {score:.3f}")
            # Plot top 5
            feats = [f for f, s in ranked_top[:5]]
            scores = [s for f, s in ranked_top[:5]]
            plt.figure(figsize=(6,4))
            plt.barh(feats[::-1], scores[::-1])
            plt.xlabel('Importance (abs(mean diff) / pooled std)')
            plt.title(f'Top features for Shot Type {st}')
            plt.tight_layout()
            plt.savefig(os.path.join(outdir, f'top_features_shottype_{st}.png'))
            plt.close()
    else:
        print("No Shot_Type_encoded column for per-shot-type analyses.")

    # 13. Save summary results to CSV summary
    summary_records = []
    for r in summary_results:
        summary_records.append({
            'model': r['model'],
            'accuracy_cv': r['accuracy_cv'],
            'roc_auc_cv': r['roc_auc_cv'],
            'avg_precision_cv': r['avg_precision_cv']
        })
    pd.DataFrame(summary_records).to_csv(os.path.join(outdir, 'models_summary.csv'), index=False)

    print("\nAll outputs saved to:", outdir)
    print("Finished.")

if __name__ == '__main__':

    main([
        "--data", "/content/basketball_shooting_dataset.csv",
        "--output", "/content/outputs"
    ])

import shutil
shutil.make_archive("/content/outputs", 'zip', "/content/outputs")